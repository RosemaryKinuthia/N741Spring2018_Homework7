---
title: "N741 Spring 2018 - Homework 7"
author: "Rosemary Kinuthia"
date: "April 6, 2018"
output:
  html_document: default
  pdf_document: default
subtitle: Homework 7 - DUE WED April 11, 2018
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(error = FALSE)
```

## Homework 7

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
helpdata <- haven::read_spss("helpmkh.sav")

h1 <- helpdata %>%
  select(age, female, pss_fr, homeless, 
         pcs, mcs, cesd)

# add dichotomous variable
# to indicate depression for
# people with CESD scores >= 16
# and people with mcs scores < 45

h1 <- h1 %>%
  mutate(cesd_gte16 = cesd >= 16) %>%
  mutate(mcs_lt45 = mcs < 45)

# change cesd_gte16 and mcs_lt45 LOGIC variable type
# to numeric coded 1=TRUE and 0=FALSE

h1$cesd_gte16 <- as.numeric(h1$cesd_gte16)
h1$mcs_lt45 <- as.numeric(h1$mcs_lt45)

# add a label for these 2 new variables
attributes(h1$cesd_gte16)$label <- "Indicator of Depression"
attributes(h1$mcs_lt45)$label <- "Indicator of Poor Mental Health"

# create a function to get the label
# label output from the attributes() function
getlabel <- function(x) attributes(x)$label
# getlabel(sub1$age)

library(purrr)
ldf <- purrr::map_df(h1, getlabel) # this is a 1x15 tibble data.frame
# t(ldf) # transpose for easier reading to a 15x1 single column list

# using knitr to get a table of these
# variable names for Rmarkdown
library(knitr)
knitr::kable(t(ldf),
             col.names = c("Variable Label"),
             caption="Use these variables from HELP dataset for Homework 07")

```

## Homework 7 Assignment

```{r}
#Load packages needed for Homework 7
library(rpart)
library(partykit)
library(reshape2)
library(party)
library(tidyverse)
library(randomForestSRC)
library(ggRandomForests)
```


### **PROBLEM 1: Regression Tree for MCS**

Using the code above, fit a regression tree model where the `mcs` is the outcome and the `cesd` is the predictor and complete the following:

* fit a regression tree to the `mcs` based on only the `cesd` scores from the `h1` dataset;
* display the results
* plot the cross-validated results
* provide a summary of the model fit
* and plot the regression tree

```{r}

# fit a regression tree model to the mcs as the outcome
# and using the cesd as the only predictor
fitmcs <- rpart::rpart(mcs ~ cesd, data = h1)
rpart::printcp(fitmcs) # Display the results
rpart::plotcp(fitmcs) # Visualize cross-validation results
summary(fitmcs) # Detailed summary of fit

# plot tree
plot(fitmcs, uniform = TRUE, compress = FALSE)
text(fitmcs, use.n = TRUE, all = TRUE, cex = 0.5)

```


### **PROBLEM 2: Matrix Scatterplot of Other Variables with MCS**

Using the code above as a guide,swap out `mcs` for `cesd` and redo the scatterplots compared to the `mcs`. HINT: You can begin with the data subset `h1a`, but you will need to modify the code for `h1m` and for the `ggplot()` code lines.

```{r}

# all vars except the dichotomous cesd_gte16 and mcs_lt45
h1a <- h1[,1:7]

# Melt the other variables down and link to mcs
h1m <- reshape2::melt(h1a, id.vars = "mcs")

# Plot panels for each covariate
ggplot(h1m, aes(x=mcs, y=value)) +
  geom_point(alpha=0.4)+
  scale_color_brewer(palette="Set2")+
  facet_wrap(~variable, scales="free_y", ncol=3)

```


### **PROBLEM 3: Regression Tree for MCS Using Rest of Variables**

Using the code above as a guide, swap out `mcs` for `cesd` and redo the regression tree for `mcs` using the rest of the variables in the data subset `h1a`. 

```{r}

# fit a regression tree with all vars
fitall <- rpart::rpart(mcs ~ ., data = h1a)

# equivalent code statement without the shorthand
# using the period for the "rest of the variables"
# this time each variable to be included is listed
# individually putting a plus + in between each 
# variable added to the model

fitall <- rpart::rpart(mcs~ age + female + pss_fr + 
                              homeless + pcs + cesd, 
                              data = h1a)

# Now let's look at fitall
rpart::printcp(fitall) # Display the results
rpart::plotcp(fitall) # Visualize cross-validation results
summary(fitall) # Detailed summary of fit

plot(fitall, uniform = TRUE, compress = FALSE, main = "Regression Tree for MCS Scores from HELP(h1) Data")
text(fitall, use.n = TRUE, all = TRUE, cex = 0.5)

```


### **PROBLEM 4: Fit a Conditional Regression Tree for MCS**

Using the code above, swap out `mcs` for `cesd` to fit a confitional regression tree for `mcs` predicted by the other variables in the dataset `h1a`.

```{r}

fitallp <- party::ctree(mcs ~ ., data = h1a)
plot(fitallp, main = "Conditional Inference Tree for MCS")
```


### **PROBLEM 5: Fit a Logistic Regression Model for MCS < 45**

The mental component (or composite) scale of the SF36 instrument is a measure of mental health. The scores are created relative to population norms. The population norm for the `mcs` of the SF36 is 50 with a standard deviation of 10. A difference of a "half" of a standard deviation - in other words a difference of 5 points - is considered to be clinically meaningful. So, people with MCS scores greater than 55 are considered to have better than average mental health and those with MCS scores less than 45 are considered to have worse than average mental health scores. So, in the dataset `h1` above, we included an indicator variable called `mcs_lt45` where a value of 1 indicates people with MCS < 45 ("poor mental health") and a value of 0 ("normal or better than normal mental health") is for people with MCS scores => 45.

Use the dataset `h1` and the code above to fit a logistic regression model for `mcs_lt45` based on the predictors of 

* `age`
* `female`
* `pss_fr`
* `homeless`
* `pcs`
* `cesd`

Is this model similar to the model for `cesd_gte16` or not - what is similar? what is different?

```{r}

# begin with a logistic regression - poor mental health or not (normal or better than normal mental health)
glm1 <- glm(mcs_lt45 ~ age + female + pss_fr + homeless + 
              pcs + cesd, data = h1)
summary(glm1)

```

```{r}
#The mcs_lt45 model does appear to be different from the cesd_gte16. In the cesd_gte16 model all the the predictors lower the cesd. However, only pcs and mcs are statistically significant. In the mcs_lt45 model, with the exception of age and pss_fr which lower the mcl, the other predictor variables increase the mcs_lt45, and only cesd is statistically significant. 
```

### **PROBLEM 6: Fit a Classification Tree for MCS < 45**

Use the `rpart` package to fit a classification tree to the poor mental health indicator `mcs_lt45`.

```{r}
fitk <- rpart::rpart(mcs_lt45 ~ age + female + pss_fr + 
                       homeless + pcs + cesd, 
                     method = "class", data = h1)
class(fitk)
# Display the results
rpart::printcp(fitk)
#Visualize the cross-validation results 
rpart::plotcp(fitk)
# Get a detailed summary of the splits
summary(fitk)
# Plot the tree
plot(fitk, uniform = TRUE, 
     main = "Classification Tree for MCS < 45 ")
text(fitk, use.n = TRUE, all = TRUE, cex = 0.8)

```


### **PROBLEM 7: Fit a Conditional Classification Tree for MCS < 45**

Using the `party` package, we can fit a conditional classification tree using the `ctree()` function. Let's do one for the indicator of depression `mcs_lt45` given the other variables in the `h1` dataset: `age`, `female`, `pss_fr`, `homeless`, `pcs`, `cesd`. 

```{r}

# look at mcs_lt45 with ctree from party
fitallpk <- party::ctree(mcs_lt45 ~ age + female + pss_fr + 
                           homeless + pcs + cesd, data = h1)
class(fitallpk)
plot(fitallpk, main = "Conditional Inference Tree for MCS < 45")

```


### **PROBLEM 8: Recursive Partitioning of Classification Tree for MCS < 45**

Using the code above to do recursive partitioning of MCS < 45 (`mcs_lt45`) on `age`, `female`, `pss_fr`, `homeless`, `pcs`, `cesd`. Also use the `partykit` package to get prettier graphics for this classification tree.

```{r}

# Recursive partitioning of MCS < 45 on age, 
# female, pss_fr, homeless, pcs, cesd
WhoHasPoorMentalHealth <- rpart::rpart(mcs_lt45 ~ age + female + 
                                 pss_fr + homeless + pcs + cesd,
                               data = h1, 
                               control = rpart.control(cp = 0.001,
                                                       minbucket = 20))

WhoHasPoorMentalHealth

library(partykit)
# Plot the tree
plot(partykit::as.party(WhoHasPoorMentalHealth))

```


### **EXTRA CREDIT Scatterplot of recursive partitions for MCS < 45 for PCS and CESD**

Using the code above, create a scatterplot of `pcs` and `cesd` where the points are colored by the indication of poor mental health `mcs_lt45`. Play with the `geom_vline()` or `geom_hline()` or `geom_segment()` to insert lines that best separate subjects with poor mental health (MCS < 45) from those with normal to better than average mental health (MCS > 45).

```{r}

# Graph as partition
# using the break points shown from the
# conditional tree
ggplot(data = h1, aes(x = cesd, y = pcs)) +
  geom_count(aes(color = mcs_lt45), alpha = 0.5) +
  geom_vline(xintercept = 24.5) +
  geom_vline(xintercept = 11.5) +
  geom_vline(xintercept = 41.5) +
  geom_segment(x = 11.5, xend = 0, y = 59.00035, yend = 59.00035) +
  geom_segment(x = 11.5, xend = 0, y = 49.7901, yend = 49.7901) +
  geom_segment(x = 41.5, xend = 0, y = 50.23704, yend = 50.23704) +
  geom_segment(x = 41.5, xend = 0, y = 54.12466, yend = 54.12466)+
  annotate("rect", xmin = 0, xmax = 100, ymin = 0, ymax = 100, fill = "blue", alpha = 0.1) +
  ggtitle("MCS < 45 Partitioned by PCS and CSD - Dark Circles normal or better than normal mental health")

```


### **PROBLEM 9: Fit a Random Forest Model for MCS**

Now let's use a Random Forest approach for modeling the MCS by the other variables in the dataset: 

* `age`
* `female`
* `pss_fr`
* `homeless`
* `pcs`
* `cesd`

Use the code above to fit the model and explore how well the model converges and how well it does predicting MCS scores.

```{r}

h1 <- as.data.frame(h1)
set.seed(131)
# Random Forest for the h1 dataset
fitallrf <- randomForestSRC::rfsrc(mcs_lt45 ~ age + female + 
                                     pss_fr + homeless + pcs + cesd, 
                                   data = h1, ntree = 100, 
                                   tree.err=TRUE)
# view the results
fitallrf
gg_e <- ggRandomForests::gg_error(fitallrf)
plot(gg_e)

# Plot the predicted cesd values
plot(ggRandomForests::gg_rfsrc(fitallrf), alpha = 0.5)

# Plot the VIMP rankins of independent variables
plot(ggRandomForests::gg_vimp(fitallrf))

# Select the variables
varsel_cesd <- randomForestSRC::var.select(fitallrf)
glimpse(varsel_cesd)

# Save the gg_minimal_depth object for later use
gg_md <- ggRandomForests::gg_minimal_depth(varsel_cesd)
# Plot the object
plot(gg_md)

# Plot minimal depth v VIMP
gg_mdVIMP <- ggRandomForests::gg_minimal_vimp(gg_md)
plot(gg_mdVIMP)

```


### **PROBLEM 10: Create Plots of How Well Each Variable Predicts CESD***

Using the code above, see how well each variable predicts MCS scores given the other variables in the dataset `h1`.

```{r}

#Create the variable dependence object from the random forest
gg_v <- ggRandomForests::gg_variable(fitallrf)

# Use the top ranked minimal depth variables only, plotted in minimal depth rank order
xvar <- gg_md$topvars

# Plot the variable list in a single panel plot
plot(gg_v, xvar = xvar, panel = TRUE, alpha = 0.4) +
  labs(y="Predicted MCS reading", x="")

```


---

The github repository for this assignment can be accessed via this link (https://github.com/RosemaryKinuthia/N741Spring2018_Homework7.git)[https://github.com/RosemaryKinuthia/N741Spring2018_Homework7.git]

---


